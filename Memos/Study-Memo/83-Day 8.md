# 83 Day 8
### 课程总结：
这节课主要是介绍Pandas包的使用以及多层神经网络中基础概念的介绍与代码实现，pandas主要讲了如何进行文件读取和一些数据结构的操作，关于神经网络主要介绍了多层神经网络，反向传播，梯度下降，损失函数等的基础知识，并讲解了如何使用基础代码进行实现，为之后的TensorFlow整合式深度学习编程语言的学习奠定了基础，之后又完整演示了如何利用人工神经元实现逻辑斯地回归的二分类功能。
### 学习总结：

| Pandas | 如何读取CSV文件，一维的series与二维的DataFrame的建立 |
| ----------------- | ------------------------------------------------------------ |
| 多层神经网络 | 神经网络的深度增加会使能力指数级增长，因此相比于增加神经元，模型的深度更能增加网络的复杂程度，但是具体也要考虑数据量的大小以及复杂程度，若使用太复杂的网络去学习太简单的数据集，则有可能造成过拟合的现象，即验证集的准确度不随时都随着训练集的准确度增大而增大，反而会收敛于某个值，并上下浮动。 |
| 损失函数 | 主要的损失函数由交叉熵（主要适用于分类问题）和MSE（主要适用于回归预测） |
| 反向传播 | 反向传播是基于梯度下降算法进行的，其主要流程就是寻找损失函数的最小值，求出相应的梯度，并基于其反方向按照一定的learning rate进行权重的更新，learning rate为调整的比例系数，即求完梯度，要关于各个参数改变多少倍的梯度。若使用整个数据集（batch）进行训练，则容易造成计算量太大的问题，因此可以将数据集分成许多个小批次（mini-batch）分别纳入到网络中训练，这样训练完一个小批次就称为一次迭代（iteration），即更新了一次权重，而当数据集中所有的mini-batch均训练完成则称为一个时代（epoch），小批次训练还能保证数据集中的噪声被抑制 |
| 逻辑斯地回归 | 采用对数似然的方法（类似交叉熵），在TensorFlow给定的具体类似事例模型框架下进行二分类问题的解决。学习率的调整：学习率太大会引起震荡，学习率太小收敛太慢 |

### 训练代码：

[Day 8]: https://github.com/Transparent-Boy/Practice-code-along-with-the-class/tree/main/Day%209

