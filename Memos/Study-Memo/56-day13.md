## week13

### 循环网络

#### 循环网络结构

##### 基本概念

- 记忆网络
- 循环网络
- 基本循环网络（RNN）
- 通过时间的反向传播算法（BPTT）
- 门结构
- 长短时记忆网络（LSTM）
- 门控循环单元网络
- 梯度截断

##### 拓展概念

- 单词嵌入向量
- 图片注解
- 文本分类
- 文本生成
- 序列对序列模型
- 神经网络机器翻译
- 注意力机制

#### 循环网络介绍

- 单个神经元->调节权重进行分类
- => 多层神经网络(深层神经网络)![image-20201208134923521](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\image-20201208134923521.png)

#### 卷积网络

- 卷积核
- 卷积运算
- 经典论文:![image-20201208135240549](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\image-20201208135240549.png)(Alex Net)
- 基本结构: 卷积层 归一化层 池化层 随机丢弃层
- 计算快 参数少 可并行

#### 循环网络

循环网络(Recurrent neural network RNN)属于反馈网络, 有时间的次序, 在时间轴上建立了一个维度

卷积核 共享权重

循环网络: 也是权重共享=> 引入反馈连接

##### 基本循环网络

![image-20201208135743607](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\image-20201208135743607.png)

x(输入层)->h(隐藏层)->a(输出层)->计算L(Loss) 一个三层全连接网络, **但是在h处引入了一个反馈W, 造成了时间上的反馈** ; 可以展开得到这样一个计算图: 

![image-20201208135956107](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\image-20201208135956107.png)

如何权重共享: U W V 在时间上看来是同一个矩阵

循环网络的计算方程: 

![image-20201208140557014](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\image-20201208140557014.png)

t时刻的a = bias + W*上一个时刻的h + U这个时刻的x

t时刻的h = tanh(这个时刻的a)

a为隐藏层的输入; h 为隐藏层的输出; 

t时刻的o = c + V这个时刻的h

t时刻的y = softmax(o这个时刻的o)

o为输出层的输入, y为输出层的输出

**tanh与softmax都是激活函数**

![image-20201208141117366](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\image-20201208141117366.png)

此处隐藏层的神经元个数为3个, 此神经网络的作用是预测接下来要输入的值是什么;

##### 循环网络的训练

![image-20201208141339785](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\image-20201208141339785.png)

两个序列直接进行对比, 得到损失进行调整; 

通过时间反向传播(BPTT)进行训练

**存在的问题**

梯度消失与梯度爆炸

![image-20201208142355278](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\image-20201208142355278.png)

##### 循环网络的变种variants

LSTM 与 GRU

双向RNN: 课后阅读

#### 长短时网络

**重点!!**

##### Gate 门结构

输入 * 控制 = 输出

![image-20201208142748290](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\image-20201208142748290.png)

Control也是一个张量, 每个位置上的值都在0-1之间

门结构可以利用反向传播来训练

##### 长短时

![img](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\FpHGzfqI134tPY6widU-pIB87qtn)

LSTM 原理: 

![img](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\Fs05ygcJuAxHqCOnkmukl4F5y26L)

增加了一条线, 3个门



Concatenate 合并 或者叫级联: 

[1 2 3] + [ 4 4 5] = [1 2 3 4 4 5]

Copy 拷贝

示例: 

[1,2,3] * [0.1,0.5,0.3] = [0.1,1,0.9]



##### 遗忘门与输入门

![img](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\Fvc6aR2QCyVQDQTVznpQoZM8b5Jb)

ft, it是张量;

##### 记忆单元与输出门

![img](C:\Users\Administrator\Desktop\learning\2020_autumn\BDMI\typora\pic\FgvRmNK3dzWECnU9XIUswbpfyuVE)



#### GRU

![img](C:/Users/Administrator/Desktop/learning/2020_autumn/BDMI/typora/pic/FvdEFTSVOESrMJsxcVRVpQRtWzaY)

只有两个门, 更新门和充值门

##### 计算方程

![img](C:/Users/Administrator/Desktop/learning/2020_autumn/BDMI/typora/pic/FpOkJ5s4i8K7gmHUI1NUa-sUnALO)

#### 循环网络应用

- 语音识别
- 图片注解
- 单词嵌入向量
- 文本分类
- 文本生成

##### 序列对序列模型

网络机器翻译NMT(Neural Machine Translation)

编码器与解码器( encoder - decoder)

![img](C:/Users/Administrator/Desktop/learning/2020_autumn/BDMI/typora/pic/FgO7_a3cr3vN6g8SvT7FAx_W0mHs)

如何实现: 

应用两个多层的LSTM网络

相关链接: 

https://github.com/tensorflow/nmt

https://github.com/tensorflow/nmt#background-on-the-attention-mechanism

注意力机制: 预测输出预测

把之前的全连接得到一个新的向量, 有一定的记忆功能

![image-20201208145359606](C:/Users/Administrator/Desktop/learning/2020_autumn/BDMI/typora/pic/image-20201208145359606.png)

transformer模型: 只利用了注意力机制, 速度很快

#### 循环网络的扩展

- 神经图灵机

使用记忆网络

### 应用部分

Tensorflow中的keras使用循环网络

![img](C:/Users/Administrator/Desktop/learning/2020_autumn/BDMI/typora/pic/Fgd6wpFVg6-tokLX0QhP5KwfJ_P6)



#### API化: 

![img](C:/Users/Administrator/Desktop/learning/2020_autumn/BDMI/typora/pic/FgZOs5hkd0HEzGT064o1q2ZAJkha)

#### tf.keras.layers.cnn

![img](C:/Users/Administrator/Desktop/learning/2020_autumn/BDMI/typora/pic/FszqO_achcR8VZAjH5QRy2KQRTeT)

需要记住的: hidden-units 模型的容量大小



##### GRU网络

GRUCell

![img](C:/Users/Administrator/Desktop/learning/2020_autumn/BDMI/typora/pic/FhCjAnihgZKeRlP5Wilzn20SggVy)

GRU层

![img](C:/Users/Administrator/Desktop/learning/2020_autumn/BDMI/typora/pic/FibaRn83ysHpIS_3sN4HT9RFwC4d)

![img](C:/Users/Administrator/Desktop/learning/2020_autumn/BDMI/typora/pic/Fl2NEiKfKbP0f5FtkIkA9O3t-9qG)

##### 推荐阅读

![img](C:/Users/Administrator/Desktop/learning/2020_autumn/BDMI/typora/pic/FiA0289lqPxjpKuGMb29XK5l7eE9)

### LSTM与RNN

![img](C:/Users/Administrator/Desktop/learning/2020_autumn/BDMI/typora/pic/Fv7Hbe_Xcsmn5jndCKvWrKq0FXoJ)



之前添加的是covd2D层, 现在是LSTM层罢了

RNN提供了Cell级API



跨批次状态: 通过stateful = True 在构造函数中进行设置来实现

