# 第十四节课：TensorFlow2高级——语音识别

#### 自动微分

##### 求导运算的方法

1、数值微分，根据定义，自变量变一小步，算比值。特点：存在误差（计算机硬件功能受限），计算效率低。

2、符号微分，将表达式视为符号运算，直接得到求导结果的解析式。特点：函数变复杂后，复杂度指数级增加，且需要大量保存中间变量。

3、自动微分：对网络中每一步的算子都用链式法则，每次都代入求出导数值传到前面的神经元中，如此只需看局部就可以得到求导信息。特点：保存了过多的中间变量用于后续计算。改进：判断哪些在之后计算需要用到，不需要的求导出的数值就扔掉。

#### TensorFlow2全景

##### 深度学习框架：

编译器：代码转换器

框架：一组软件来完成操作

中间表示（IR）：编译器经常用，比如代码跑在各种平台上，则需要有中间结果，再部署到各个平台上。

元编程：写一段程序用于生成程序。

自动微分原理：在计算完输出后，从计算的终点一个个往回计算，生成一个反过来的计算图。

编程模型：如何从软件界面部署到硬件模型上？

上层：训练库和推断库；中层：Python和C++接口；底层：网络层和设备层，允许多个设备并行。

深度学习框架是描述多层网络模型和模型训练推断的程序语言及工具类库，也即TensorFlow包，其同申明式语言（比如SQL）更类似。其包括：编程语言、解释器、编译器、动态计算图和静态计算图。框架的不同其实对应程序语言内部的不同设计。

TensorFlow由Google开发，pyTorch由Facebook开发。

TensorFlow会自动求导AD，优化权重参数，使得损失函数最小，可以称为一种“可求导编程语言”。TensorFlow的硬件优化编译器XLA(Accelerater Linear Algebra)优化TensorFlow计算图的实现。

##### 深度学习编译器

其连接了软件框架与硬件加速器。软硬协同设计：硬件专门用于执行某些计算（算子库），最后集成为了“AI芯片”。

##### MLIR

多级中间表示，是表示形式和编译器实用程序的库，位于模型表示与机器代码之间，用于优化编译器基础架构，由中间形式和代码库组成。

LLVM和TVM是常用的MLIR框架，有待日后详细了解。

#### 语音识别原理及应用

##### 语音识别的对象与本质

语音信号是声波，其可用波形图（时变信号）、频率图（FFT后）、时频谱（在波形图中拿很小的窗口，对小窗口进行傅里叶变换，再让小窗口不断向前，最后叠加在一起）来表示。

本质上，v=f(t,h1)，h1为隐状态，表示人的口音等，w=F（v，h2）,h2是含有上下文的隐状态。其中v为语音信号，w为文字输出。

##### 语音识别评判指标

词错误率（WER）

错误率为对齐标准答案和识别结果，需要多少单个词的变换才能由结果变为标准答案。

#####  语音识别技术发展

传统方法：对指定语音信号X和单词W，最优的识别结果W\*可表述为：$W*=arg max P(W|X)$（再拿贝叶斯换P（W|X））。

因而经典过程为语音信号经过FFT，变为向量，再变为向量序列，变为音素序列，变为字母序列，变为词汇序列。

然后由一系列技术最后得到了识别。

而在深度学习中只需要训练数据然后参数优化就可以了，称为合金模型，即深度神经网络与经典方法的模块混合，但混合模型比较复杂，需要复杂的训练方法。最后采用了纯深度学习，使错误率继续下降。

谷歌CLDNN，结合CNN,LSTM和DNN：语音进来先卷积，再循环，再全连接。具体参见ppt，可以用于作为大作业的参考。

#### CTC技术（Connectionist temporal classification）

 ASR(自动语音识别技术)之前效率不高，之后有了CTC技术，RNN开始处理语音识别了，即预测语音和文字对齐的条件概率问题。

CTT：RNN做条件概率预测。

不需要再在声音上做任何标注，输入语音直接可以输出文字，为端到端系统也做好了准备。

CTC要解决的问题：挑战在于输入语音和输出文字之间，X和Y都是变长的，没有一种确定的X和Y的对齐方式。

输入语音X=[x1,x2,...,xT]，输出文字Y=[y1,y2,...,yU]

流程(CTC折叠)：先生成一个序列，再合并相同的字符，扔掉空字符，最后合在一起，得到输出的单词。

而对一个时频谱图，对每一个小时间段进行划分，每一段都进行识别，在可能的字符中用该段可能出现的字符概率表示，概率最大的代表这一段的字符，最后把概率最高的字符串在一起得到序列。

##### DeepSpeech2（端到端语音识别系统实例）

输入时频谱图，输出英文输出或者中文输出的句子。

其模型组成也是先卷积，再循环，最后全连接输出。

##### 想自己搭建？也可以！

现有的语料库:https://tv.tsinghua.edu.cn/

模型：DeepSpeech2/3

然后再有一些GPU等计算资源，我们也可以自己完成语音识别系统的搭建



### 课程大作业

每个文件夹内相同的指令，是不同人说的，共24句语音指令。

我们需要用Tensorflow搭建神经网络，输出为24个的softmax输出。

声音数据增强：SOX程序。

ffmpeg程序用于统一转换为.wav文件。

定义：例如训练集有100张照片，分为50个子集，每个子集，每个子集2张，则Batch_size=2,若epoch = 10,则iteration=500。

epoch：一遍batch全喂完了。

iteration：每次子集投喂后都会进行一次参数调整，每次调整为网络的一次迭代。