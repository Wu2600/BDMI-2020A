# 第十三周学习总结

### 本次课程的内容有：

1. 循环神经网络理论

- 循环神经网络

  - 基本结构：在时间维度上，每一个时间步处理时，采用相同的权重（weight sharing）

  - 基本循环网络（vanilla RNN）：全连接网络+时间维度=>隐藏层上一时刻的状态会影响这一时刻的状态。

    问题：收敛慢，梯度消失或梯度爆炸

    每个时刻的隐藏层利用全连接与下个时刻的隐藏层相连，不同时间上U, V, W相同，且均为全连接

    ![image-20201208140030802](C:\Users\13693\AppData\Roaming\Typora\typora-user-images\image-20201208140030802.png)

    循环网络的正向计算方程：

    ![image-20201208140707178](C:\Users\13693\AppData\Roaming\Typora\typora-user-images\image-20201208140707178.png)

  - 循环网络的变种：LSTM解决基本 循环网络收敛慢的问题，GRU解决LSTM计算慢的问题(适用于实时计算如语音输入)

  - 双向RNN：语音识别，手写识别

- 长短时记忆网络(LSTM)

  - 门结构（Gate）：遗忘门、输入门、记忆单元、输出门
  - ![image-20201208144007089](C:\Users\13693\AppData\Roaming\Typora\typora-user-images\image-20201208144007089.png)
  
- GRU门控循环单元

  - 更新门：合并了LSTM输入门和遗忘门

    重置门

    合并记忆状态和隐藏状态为一个状态

    ![image-20201208144217002](C:\Users\13693\AppData\Roaming\Typora\typora-user-images\image-20201208144217002.png)

- 序列对序列模型：机器翻译->编码器、解码器结构

  序列输出预测：注意力机制，利用前一段时间的信息，而不是前一个时间——Transformer模型，去掉RNN序列结构，优点是计算快，完全基于attention和全连接
  
- 循环网络扩展：神经图灵机 differentiable neural computers（DNC architecture）

2. keras进行RNN

- RNNCell：一个时间步内，h层的结构

- tf.keras.layers.rnn:

  - hidden-units: 模型的容量大小，即h层中有几个神经元

  - 可以使用的cell的API：

    ![image-20201208151610796](C:\Users\13693\AppData\Roaming\Typora\typora-user-images\image-20201208151610796.png)

  - tf.keras.layers.LSTMCell (return_sequence可以把参数返回，用于attention)，以及tf.keras.layers.lstm
  
  - tf.keras.layers.GRUCell，tf.keras.layers.GRU
  
3. tf.data


### 课程中遇到的问题：

暂无

### 课程收获：

学习了RNN网络的基本知识，对于其使用场景和keras中的实现方式产生了了解，同时学习到了如何将本地文件导入形成数据集