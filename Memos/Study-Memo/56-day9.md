## 第9节课

### Tensor的创建

```python
#标量
rank_0_tensor = tf.constant(4)
print(rank_0_tensor)
#一维向量
rank_1_tensor = tf.constant([1,2,2])
print(rank_1_tensor)
#3行2列的矩阵
rank_2_tensor = tf.constant([[1,2],[3,4],[5,6]],dtype = tf.float16)
#3轴张量
rank_3_tensor = tf.constant([
    [[0,1,2,3,4],
    [5,6,7,8,9]],
    [[10,11,12,13,14],
    [15,16,17,18,19]],
    [[20,21,22,23,24],
    [25,26,27,28,29]],
])
```

### Tensor的运算

```python
tf.add()
tf.multiply()
tf.matmul()
#张量运算符: 
tf.reduce_max()
tf.argmax(c)
tf.nn.softmax(c)

```

### Tensor形状

### Tensor索引

索引从0开始, 冒号用于切片, 索引为左闭右开;

负索引表示按照倒序编制索引

#### 单轴切片

#### 多轴切片

#### Tensor的reshape

#### Tensor广播

为了适应大张量, 会对小张量进行拓展

#### 字符串张量

#### 稀疏张量

```python
#稀疏张量
sparse_tensor = tf.sparse.SparseTensor(indices = [[0,0],[1,2]],
                                      values = [1,2],
                                      dense_shape= [3,4])
print(sparse_tensor)
#SparseTensor(indices=tf.Tensor(
#[[0 0]
# [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))
print(tf.sparse.to_dense(sparse_tensor))
#tf.Tensor(
#[[1 0 0 0]
# [0 0 2 0]
# [0 0 0 0]], shape=(3, 4), dtype=int32)
```

### TensorFlow 变量

### TensorFlow自动微分

```python
# 自动微分(tf.GradientTape API)
x = tf.ones((2,2))
with tf.GradientTape() as t:
    t.watch(x)
    y = tf.reduce_sum(x)
    z = tf.multiply(y,y)
#dz_dx = t.gradient(z,x)
#print(dz_dx)
#for i in [0,1]:
#    for j in [0,1]:
#        assert dz_dx[i][j].numpy() == 8
dz_dy = t.gradient(z,y)
print(dz_dy)
assert dz_dy.numpy() == 8
# 自动微分(tf.GradientTape API)
x = tf.ones((2,2))
with tf.GradientTape() as t:
    t.watch(x)
    y = tf.reduce_sum(x)
    z = tf.multiply(y,y)
#dz_dx = t.gradient(z,x)
#print(dz_dx)
#for i in [0,1]:
#    for j in [0,1]:
#        assert dz_dx[i][j].numpy() == 8
dz_dy = t.gradient(z,y)
print(dz_dy)
assert dz_dy.numpy() == 8
tf.Tensor(8.0, shape=(), dtype=float32)
# 持续的梯度带
x = tf.constant(3.0)#为什么需要是float
with tf.GradientTape(persistent = True) as t:
    t.watch(x)
    y = x*x
    z = y*y
dz_dx = t.gradient(z,x)
dz_dy = t.gradient(z,y)
print(dz_dx)
print(dz_dy)
del t
tf.Tensor(108.0, shape=(), dtype=float32)
tf.Tensor(18.0, shape=(), dtype=float32)
1
def f(x):
    output = 1.0
    output = tf.multiply(x,x) + tf.exp(x)
    return output
x = tf.convert_to_tensor(1.0)
with tf.GradientTape(persistent = True) as t:
    t.watch(x)
    y  = f(x)
dy_dx = t.gradient(y,x)
print(dy_dx)
tf.Tensor(4.7182817, shape=(), dtype=float32)
d2y_dx2
#计算高阶微分
x = tf.Variable(1.0)
with tf.GradientTape() as t:
    with tf.GradientTape() as t2:
        y = x*x*x
    dy_dx = t2.gradient(y,x)
d2y_dx2 = t.gradient(dy_dx,x)
print(d2y_dx2)
tf.Tensor(6.0, shape=(), dtype=float32)
```

## TensorFlow 即刻执行

即刻求值与惰性求值

**=>即刻执行**

```python
x = [[2.]]
m = tf.matmul(x,x)
print("hello,{}".format(m))
#hello,[[4.]]
```

**即刻训练**

```python
#即刻训练
w = tf.Variable([[1.0]])
with tf.GradientTape() as tape:
    loss = w*w
grad = tape.gradient(loss,w)
print(grad)
tf.Tensor([[2.]], shape=(1, 1), dtype=float32)
```

