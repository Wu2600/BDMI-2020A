# 课程小结 LESSON 13

## 一、RNN循环神经网络

1. 循环神经网络：

   1. 介绍

   - 类似全连接神经网络，但是h(t)和x(t)、h(t-1)均有关

   - <img src="C:\Users\吴奕\AppData\Roaming\Typora\typora-user-images\image-20201208140714561.png" alt="image-20201208140714561" style="zoom: 25%;" />

   - <img src="C:\Users\吴奕\AppData\Roaming\Typora\typora-user-images\image-20201208140810132.png" alt="image-20201208140810132" style="zoom: 50%;" />

   - ==从下往上看是个全连接。只是在隐藏层计算时需要考虑到上一时刻传来的隐藏层，其实就是全连接+时间维度的考虑==

     - 循环神经网络示例1：
       - 利用RNN实现预测，如下图所示：
       - <img src="C:\Users\吴奕\AppData\Roaming\Typora\typora-user-images\image-20201208141231156.png" alt="image-20201208141231156" style="zoom:33%;" />
       - 对于字母，这里采用了One-hot编码的方式

     2. 循环网络的训练：
        - BPTT一次向前传播（左到右），一次向后传播（右到左）
        - Gradient Clipping
          - 如：任何梯度值大于1的时候，就把梯度固定为1，防止出现梯度爆炸的问题
     3. 循环神经网络的变种：LSTM（长短记忆法）; GRU（GRU是简化LSTM，缩短了LSTM的计算时长）

2. 双向循环网络：

   1. 适用于语音识别，可后续阅读

3. LSTM:（可以让RNN收敛更快）

   - 门结构：inpu t* control = output
   - <img src="C:\Users\吴奕\AppData\Roaming\Typora\typora-user-images\image-20201208142832725.png" alt="image-20201208142832725" style="zoom:33%;" />
   - 控制经过sigmoid变换之后变成一个张量，这个张量每个单元都是处于0~1之间的数据
   - 控制点和输入之间是两两相乘，不是点乘。
   - **遗忘门**：f(t)*C(t-1) 代表对于上一时刻的部分遗忘
   - **输入门**：i(t) * C~(t) 是这一时刻新的输入部分。
   - 遗忘门+输入门 之后 才真正更新当前状态的C(t)

4. GRU：（LSTM的简化）

   -  只有两个门：更新门+重置门

5. ==注意力机制== （attention mechanism):

   - 实例：Transformer（google）
     - 该模型不包括循环，增加了位置编码，使得训练更快（不需要一个一个的输入训练）

6. 图灵机

## 二、循环神经网络TF搭建

1. SimpleRNNcell （最简单的全连接RNN）
   - 不断建立simpleRNNcell，至少所有输入都输入完。
2. Bidirectional （双向循环网络）
3. LSTM cell
   - 直接利用tf.keras.layers.LSTM(...) 建立
4. 跨批次状态
   - 使用跨批处理有状态性的模式处理**长序列**
   - 相当于把文本/语音切割成一批一批较短的序列来处理

## 三、tf.data的使用

1. 对类别不平衡的数据处理：
   1. tf.data.experimental.sample_from_datasets