## 第九节课：Tensorflow实践

#### 用张量表示神经网络

可以把输入每层网络的参数都视为向量/矩阵/张量，如X,W1,H,W2,N,Y等等

此时每层的运算可以用一个线性映射加一个偏置再加一个非线性激活函数表示。

#### TesnorFlow2基础

##### 张量

与元组类似，tensorflow里的张量是不可变常量，创建时用tf.constant(x)。

运算和索引也和元组的类似。

```python
import tensorflow as tf
rank_0_tensor = tf.constant(4)
print(rank_0_tensor)

#找到最大的元素的值与索引
print(tf.reduce_max(a))
print(tf.argmax(a))

reshaped = tf.reshape(a,[2,2])
f16_tensor = tf.cast(a,dtype=tf.float16)

#广播，即自动扩展为张量
x=tf.constant(2)
print(x*a)

#不规则张量
ragged_list = [
    [0,1,2,3],
    [1,2,3]
]
ragged_tensor = tf.ragged.constant(ragged_list)
#字符串张量
tf.string.split(x)
#稀疏张量
sparse_tensor = tf.sparse.SparseTensor(indices=[[0,0],[1,2]],
                                      values=[1,2],
                                      dense_shape=[3,4])
```

##### 变量

tf.Variable表示变量，对其执行运算可以改变其值，可以存储模型参数。

```python
#查看变量存储位置
print(tf.debugging.set_log_device_placement)

my_tensor = tf.constant([[1.0,2.0],[3.0,4.0]])
my_variable = tf.Variable(my_tensor)
print(my_variable.shape,'\n',my_variable.dtype,'\n',my_variable.numpy)

print(tf.argmax(my_variable))
print(tf.argmax(my_variable[0]))

print(tf.convert_to_tensor(my_variable))
#注：变量无法改变形状，reshape只会新建一个张量

#变量可以通过assign更改数值，但无法改变数据类型与大小
my_tensor = tf.constant([1,2,3])
my_variable = tf.Variable(my_tensor)
my_variable.assign([4,5,6])

#assign的其他用法
print(my_variable.assign_add([1,1,1]).numpy())
print(my_variable.assign_sub([1,1,1]).numpy())
#要是不想求微分，可以把梯度关闭
step_counter = tf.Variable(1 , trainable=False)
```



##### 自动微分

通过梯度带实现：

```python
#自动微分
x = tf.ones((2,2))

with tf.GradientTape(persistent=True) as t:#要是没有persistent，则计算完一次之后就释放资源了
    t.watch(x)
    y = tf.reduce_sum(x)
    z = tf.multiply(y,y)
    
dz_dx = t.gradient(z,x)
for i in [0,1]:
    for j in [0,1]:
        assert dz_dx[i][j].numpy() == 8.0
#--------------------------------------------------------------------        
x = tf.constant(1.0)

with tf.GradientTape(persistent=True) as t:
    t.watch(x)
    y = tf.multiply(x,x)+tf.exp(x)
    
dy_dx = t.gradient(y,x)
print(dy_dx)

#高阶导数，可以创建两个梯度带，然后在内层梯度带里先进行求导，作为一个新的函数送到外层梯度带中。
```

##### 即刻执行 

tensorflow2.0的新增功能，可以使得在模型执行时，python所有功能都是可以使用的。

即刻求值与惰性求值的差别：即刻求值是在函数的所有参数都算出来之后再计算函数值，是目前我接触到的语言的运算规则。

而惰性求值，除非函数要调用这个参数，否则这个参数永远不会被计算。