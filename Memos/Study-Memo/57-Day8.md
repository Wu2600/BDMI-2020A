# 第八次课程报告

### Pandas学习

基于Numpy的分析结构化数据的工具集，用于数据挖掘和数据分析，也提供数据清洗功能。

- 两种数据基础
  - Series：一维数据结构，它的索引叫index，其data可为字典，ndarray 或标量。
    简单的向量操作与ndarray表现一致，不同：Series的操作默认使用index的值进行对齐，而不是相对位置。
  - DataFrame：二维数据结构，其data可为：
    - 一个储存一维数组，字典，列表或Series的字典
    
    - 2-D数组
    
    - 结构或记录数组
    
    - 一个Series
    
    - 另一个DataFrame
  
### 多层神经网络(Deep)

人工神经元进行二元分类采用逻辑斯提模型。

输入层->隐藏层->输出层->输出值（归一化为一个概率，softmax处理）

- 网络层数越多：

  - 一方面，表达能力增强
  - 另一方面， 权重数量增加

- 自动化的权重确定（也称为神经网络的训练）

  - 损失函数(Loss)（从优化的角度）

    Y为实际输出，Y'为标签，即预期输出

    度量函数D(y,y')度量y与y'差异，计算出差异值Delta

    如平方和、均方差、交叉熵等。

    训练目标即通过调整神经网络内部权重，使Delta值最小化的过程

    用度量函数来表示这种差异，叫做损失函数或成本函数（Cost）

    - 方差（variance）：各数据与平均数之差的平方值的平均数

    - 标准差（standard deviation）：方差开方，亦称均方根

    - 均方误差（mean square error，MSE）：误差平方值的平均数

    - 均方根误差（root mean square error，RMSE）：均方误差开方

    - 交叉熵（Cross Entropy，CE）：是负对数似然损失函数

$$
H_{y'}(y)=-\Sigma_i{y'_i}log(y_i)
$$

      J = Loss = D(y,y')

- 梯度下降法（Gradient descent）
  
    也称为最速下降法（steepest descent），是一个最优化算法
  
    过程：要找到一个函数的局部极小值，必须向函数上当前点对应梯度的反方向，按照规定步长距离点，进行迭代搜索。
  
    反向传播算法：
  
    - 根据损失函数的性质及链式求导法则
    - 反向逐层求导损失函数对权重的梯度（各个偏导数）

- 权重更新（随机梯度下降法）

  - 梯度算子不断调整权值θ，Loss不再有较大变化，就认为是收敛了。

  - 步长又称为学习率（learning rate），一般为0.01

    （学习率过大会引起震荡，过小会收敛太慢）
    
$$
\theta=\theta-\eta\nabla_{\theta}J(\theta)
$$

- 随机梯度下降法（stochastic gradient descent，SGD）是最常用的权重调节方法，步骤如下：

    - 1、随机初始化每个神经元输入权重和偏差
    - 2、选取一个随机样本
    - 3、根据网络输出结果，从最后一层开始后，逐层计算每层权重的偏导数
    - 4、逐层调整权重，产生新的权重值
    - 回到第二步
  核心为一个“随机样本”
  
- 神经网络训练实际过程

    “分批训练”优点收敛性有保证，缺点更新模型参数时，需要遍历整个数据集

    - 整个训练集成为一个批次（batch）

    - 将整个训练集分成多个大小相同子集，每个子集成为一个迷你批次（mini-batch），大小由mini-batch_size控制
    - 训练完一个迷你批次，被称为一次迭代（iteration）
    - 训练集中所有训练样本都被送入网络，完成一次训练的过程称为一个时代（epoch）
### 用张量表示神经网络

张量（Tensor）的实质是一个N维数组

Tensor的Numpy表示Ndarray

例：对一个4 * 5 * 6的张量

rank：3d             length：4，5，6             shape：[4，5，6]             volumn：4 * 5 * 6 = 120

亦即TensorFlow名称的来历。

### TensorFlow Basic

计算图（Graph），即时执行（Eager Execution）

Tensor有Constant，Placeholder，Variable三种表现形式

宽度or深度？深度能提取信息的更多属性，但不能一味增加深度，否则可能产生过拟合的问题。