# 36-Day13

### 1、循环网络（RNN）

#### （1）多层人工神经网络的层间连接-反馈网络

#### （2）循环网络基本结构

##### 循环网络的特点：

在时间维度上，每一个时间步处理时，采用相同的权重

即权重共享

##### 基本循环网络（vanilla RNN）：

###### 网络结构：

训练目标：y

损失函数：L

网络输出层：o

网络隐藏层：h

网络输入层：x

###### 反馈连接：

隐藏层的当前状态，会反馈到下一个时间步的状态

##### 权重共享（时间上）：

###### 在不同的时间步上，采用相同的U、V、W权重矩阵

U：输入层到隐藏层，全连接的权重矩阵

V：隐藏层到输入层，全连接的权重矩阵

W：隐藏层到隐藏层，全连接的权重矩阵

##### 循环网络的计算方程：

$$
a^{(t)}=b+Wh^{(t-1)}+Ux^{(t)}
$$

$$
h^{(t)}=tanh(a^{(t)})
$$

$$
o^{(t)}=c+Vh^{(t)}
$$

$$
y^{(t)}=softmax(o^{(t)})
$$

##### 损失函数：

###### 循环网络的训练损失：

将输入序列（x）映射到对应的输出序列（o）

###### 损失L：

衡量每个输出o与相应的训练目标y的距离

##### 梯度截断：

训练循环网络时，经常出现梯度要么太大，要么太小，为了加速训练，需要把梯度设置为一些固定数值。

比如说，梯度的任何维度的数值应该小于1，如果某个维度的数值大于1，则固定设置为1.

##### 循环网络的变种variants：

###### LSTM（长短时记忆网络）：

解决基本循环网络收敛慢的问题

###### GRU（门控循环单元）：

简化LSTM，解决LSTM计算成本高的问题

#### （3）双向循环网络

#### （4）长短时记忆网络

##### 门结构（Gate）：

输入和控制是形状一致的张量

控制经过Sigmoid函数后，变成一个范围在0-1之间的一个同形状的张量

输入和控制元素相乘，可以得到一个同形的输出

门结构可以反向传播训练

##### LSTM原理：

记忆单元：c

遗忘门：f

输入门：i

输出门：o

#### （5）GRU

##### 改进：

###### 两个门：更新门、重置门

###### 一个状态：

合并LSTM的记忆状态c和隐藏状态h为一个状态

#### （6）循环网络应用

#### （7）序列对序列模型



### 2、Keras RNN

