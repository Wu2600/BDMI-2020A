# 第十三节课：循环网络

#### 循环网络

##### 基本概念

记忆网络

循环网络：反馈网络

基本循环网络（Vanilla RNN）

通过时间反向传播算法（BPTT）

--------------------------

门结构

长短时记忆网络（LSTM）

门控循环单元网络

-----------------------------

梯度截断：当梯度比较大时，强行设成固定值，防止过大

##### 拓展概念

单词嵌入向量（单词放在向量里）

图片注解（一张图片上自动分类）

文本分类（好评/差评？）

文本生成（写诗）

序列对序列模型（Seq2Seq）（输入序列与输出序列，两个循环网络）

神经网络的机器翻译（NMT）

注意力机制

图灵机

神经图灵机

可微神经计算机



##### 循环网络的特点

时间维度上，每一个时间步处理时，采用相同的权重。

##### 基本循环网络（vanilla RNN）

隐藏层当前的状态会反馈到下一个时间步的状态。

计算方程：

$a^t = b+Wh^{t-1}+Ux^t$

$h^t = tanh(a^t)$

$o^t =c +Vh^t$

$y^t = softmax(o^t)$

核心思想：今天的天气取决于昨天的天气，前面的状态对后面的状态有影响。

E.A 预测hello中下一个字母是什么

##### 损失函数

将输入序列映射到输出序列o，将o与期望的输出序列y进行距离计算（实际操作中可以用softmax计算完再计算距离。）

##### 通过时间反向传播（BPTT）

每一个时间都调整一次权重。

缺点：每个时间步只能一前一后地计算，内存代价是O（T），训练也很慢。

其他问题：BPTT训练RNN时会出现梯度消失和梯度爆炸的问题：

1、最初和最后的时间步的梯度过大或过小，可用梯度截断



循环网络的变种：

#### 长短时记忆网络（LSTM）

基本单元LSTMCell

LSTM解决基本循环网络收敛慢的问题

门控循环单元网络（GRU），简化LSTM，解决LSTM的计算成本高的问题

#####  门结构

定义：输入与控制、输出是形状一致的张量。控制经过sigmoid函数，变为一个0~1之间的张量。输入与控制元素对应位乘，再输出。



增加了一个记忆单元，增加了输入门、遗忘门、输出门

（具体结构待整理，主要思想是存在门用于控制C与h量前系数的计算）



#### GRU（折中）

GRU合并了输入门和遗忘门，用于控制历史信息的影响；同时还有重置门，可以忽略历史信息。

#### 双向循环网络：

一个由之后的时间决定，一个由之前的时间决定，有双向的时间序列。

#### 循环网络的应用

即基于序列建模的预测问题：

手写识别（词与词之间共同识别）

诗歌填词、对联撰写、代码生成

股价预测、天气预测

语音识别、图片注释、机器翻译

-------

语音识别ASR、图片注解、单词嵌入向量、文本分类

##### 神经网络机器翻译NMT

编码器将输入语句翻译为中间向量，解码器将中间向量翻译为翻译的结果。

注意力机制：

在Seq2Seq中第二个LSTM网络的输入是之前权重最高的词（贪心搜索），但实际效果不好

所有时候的输出的最大值记录下来都带到后面算，称为注意力机制。

#### 循环网络的扩展

##### 神经图灵机

用神经网络模拟图灵机？

可微神经计算机（DNC）



### Keras RNN的API

Cell层：一个时间步上的运算，就像晶格一样。

```python
tf.keras.layers.rnn
```

input+state -> output+new_state

注：hidden-units:模型容量大小：隐藏层有几个神经元

state：隐含了之前所有的输出信息

#### 简单RNN网络

```python
#例程
#简单RNN网络
inputs = np.random.random([32,10,8]).astype(np.float32)
rnn = tf.keras.layers.RNN(tf.keras.SimpleRNNCell(4))

output = rnn(inputs)

rnn = tf.keras.layers.RNN(
	tf.keras.layers.SimpleRNNCell(4),
    return_sequences = True,
    return_state=True
)

whole_sequence_output , final_state = rnn(inputs)
#whole_sequence_output 大小为[32,10,4]
#final_state 大小为[32,4]
```

#### 双向循环网络

```python
tf.keras.layers.Bidirectional(
  layer,merge_mode='concat',weights=None,backward_layer=None
)
```

#### LSTM（实际应用最广）

```python
inputs = tf.random.normal([32,10,8])
rnn = tf.keras,RNN(tf.keras.layers.LSTMCell(4))
output = rnn(inputs)

rnn = tf.keras.layers.RNN(
	tf.keras.layers.LSTMCell(4),
    return_sequences=True,
    return_state=True
)
whole_seq_output , fianl_memory_state , fianl_carry_state = rnn(inputs)
```

#### GRU

```python
inputs = tf.random.normal([32,10,8])
rnn = tf.keras,RNN(tf.keras.layers.LSTMCell(4))
output = rnn(inputs)

rnn = tf.keras.layers.RNN(
	tf.keras.layers.GRUCell(4),
    return_sequences=True,
    return_state=True
)
whole_seq_output , fianl_state = rnn(inputs)
```

