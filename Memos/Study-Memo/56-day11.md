## 第11节课

### Keras框架

https://keras.io/

提供了很多创建神经网络用的API, 大大方便了人们去使用它

学习Keras, 顺序模型, 函数API, 训练与评估

深度学习3: 自动微分, 数值计算

深度学习4: 网络架构

#### Keras总体介绍

一句话搞定dense 激活函数等, 是个封装的十分好的框架

#### 预备知识: 网络结构

**多层神经网络**

feedforward: 单向, 一直往前

feedback: 上一个时刻的输出会在下一个时刻输入

memory network : 可以记住前面输出的任何内容.

多层前馈网络: 

卷积网络 循环网络: 

#### Keras框架核心概念

Keras是一套高级API, 用来快速搭建深度神经网络. 

##### 模型

核心的数据结构是模型, 模型是组织层的一种方法, 包括架构和权重参数

最简单的是顺序模型: 串接很多层的线形管道

##### 层

tf.keras.layers

层分为: Dense Activation(激活层) Dropout(随机丢掉) Flatten(压平) 

Convolutional Layers(卷积网络用的), Pooling Layers Normalization Layers(归一化层)

##### 激活函数

tf.keras.activations(涵盖了常用激活函数)

softmax elu softplus ==relu== sigmoid tanh softsigh hard_sigmoid linear

表示式: softmsax(x), relu(x, alpha = 0.0, max_value = None) sigmoid(x)等

##### 优化器

动量参数: 给它一个动量, 让它能够跳出局部最优点



Nadam采用加速梯度算法的优化器

基于之前所有的梯度值更新模型参数, 而不限于当前局部的梯度.

#### Keras 的使用流程

练手的数据集: mnist数据集("hello world"练手数据集)

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

fashion-mnist数据集(代表平时的衣物)

前6万张用于训练, 后1万张用于测试



先定义网络, 再编译网络

再进行网络的训练

batch_size : 一次训练几个

nb_epoch:训练几个时代(10个时代表示每个数据都要你刚刚10次)

保存网络: 使用model.save() 或者tf.keras.models.save_model()

**示例: **

```python
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='elu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=10)

model.evaluate(x_test,  y_test, verbose=2)

```

### Python面向对象编程







### Keras的层和模型

#### Layer类



#### Keras顺序模型

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(784,), name="digits")
x = layers.Dense(64, activation="relu", name="dense_1")(inputs)
x = layers.Dense(64, activation="relu", name="dense_2")(x)
outputs = layers.Dense(10, activation="softmax", name="predictions")(x)

model = keras.Model(inputs=inputs, outputs=outputs)

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Preprocess the data (these are NumPy arrays)
x_train = x_train.reshape(60000, 784).astype("float32") / 255
x_test = x_test.reshape(10000, 784).astype("float32") / 255

y_train = y_train.astype("float32")
y_test = y_test.astype("float32")

# Reserve 10,000 samples for validation
x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

model.compile(
    optimizer=keras.optimizers.RMSprop(),  # Optimizer
    # Loss function to minimize损失函数
    loss=keras.losses.SparseCategoricalCrossentropy(),
    # List of metrics to monitor指标
    metrics=[keras.metrics.SparseCategoricalAccuracy()],
)
#model.compile(
#optimizer = 'rmsprop',
#loss='sparse_categorical_crossentropy'
#metrics=["sparse_categorical_accuracy"])

print("Fit model on training data")
history = model.fit(
    x_train,
    y_train,
    batch_size=64,#一次训练几批
    epochs=2,#几个时代
    # We pass some validation for
    # monitoring validation loss and metrics
    # at the end of each epoch
    validation_data=(x_val, y_val),
)

history.history

# Evaluate the model on the test data using `evaluate`
print("Evaluate on test data")
results = model.evaluate(x_test, y_test, batch_size=128)
print("test loss, test acc:", results)

# Generate predictions (probabilities -- the output of the last layer)
# on new data using `predict`
print("Generate predictions for 3 samples")
predictions = model.predict(x_test[:3])
print("predictions shape:", predictions.shape)

model.compile(
    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),
    loss=keras.losses.SparseCategoricalCrossentropy(),
    metrics=[keras.metrics.SparseCategoricalAccuracy()],
)

model.compile(
    optimizer="rmsprop",
    loss="sparse_categorical_crossentropy",
    metrics=["sparse_categorical_accuracy"],
)

#进阶的神经网络
def get_uncompiled_model():
    inputs = keras.Input(shape=(784,), name="digits")
    x = layers.Dense(64, activation="relu", name="dense_1")(inputs)
    x = layers.Dense(64, activation="relu", name="dense_2")(x)
    outputs = layers.Dense(10, activation="softmax", name="predictions")(x)
    #使用softmax进行分辨
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model


def get_compiled_model():
    model = get_uncompiled_model()
    model.compile(
        optimizer="rmsprop",
        loss="sparse_categorical_crossentropy",
        metrics=["sparse_categorical_accuracy"],
    )
    return model

```

优化器：

- `SGD()`（有或没有动量）
- `RMSprop()`
- `Adam()`
- 等等

损失：

- `MeanSquaredError()`
- `KLDivergence()`
- `CosineSimilarity()`
- 等等

指标：

- `AUC()`
- `Precision()`
- `Recall()`
- 等等

##### 自定义损失

有两种方法来使用 Keras 提供自定义损失。

方法一:

第一个示例创建一个接受输入 `y_true` 和 `y_pred` 的函数。下面的示例显示了一个计算实际数据与预测值之间的均方误差的损失函数：

```python
def custom_mean_squared_error(y_true, y_pred):
    return tf.math.reduce_mean(tf.square(y_true - y_pred))
#尽量用tf的算子以保证稳定性
model = get_uncompiled_model()
model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)#在loss这里进行了调用

# We need to one-hot encode the labels to use MSE
y_train_one_hot = tf.one_hot(y_train, depth=10)
model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)
```

方法二

使用除 `y_true` 和 `y_pred` 之外的其他参数的损失函数，则可以将 `tf.keras.losses.Loss` 类子类化，并实现以下两个方法：

- `__init__(self)`：接受要在调用损失函数期间传递的参数

- `call(self, y_true, y_pred)`：使用目标 (y_true) 和模型预测 (y_pred) 来计算模型的损失(会默认调用call函数)

  ```python
  class CustomMSE(keras.losses.Loss):
      def __init__(self, regularization_factor=0.1, name="custom_mse"):
          super().__init__(name=name)
          self.regularization_factor = regularization_factor
  
      def call(self, y_true, y_pred):
          mse = tf.math.reduce_mean(tf.square(y_true - y_pred))
          reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))
          return mse + reg * self.regularization_factor
  
  
  model = get_uncompiled_model()
  model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())
  
  y_train_one_hot = tf.one_hot(y_train, depth=10)
  model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)
  ```

通过添加层计算损失, 是最灵活的一种





```python
class ActivityRegularizationLayer(layers.Layer):
    def call(self, inputs):
        self.add_loss(tf.reduce_sum(inputs) * 0.1)
        return inputs  # Pass-through layer.
    
inputs = keras.Input(shape=(784,), name="digits")
x = layers.Dense(64, activation="relu", name="dense_1")(inputs)

# Insert activity regularization as a layer
x = ActivityRegularizationLayer()(x)

x = layers.Dense(64, activation="relu", name="dense_2")(x)
outputs = layers.Dense(10, name="predictions")(x)

model = keras.Model(inputs=inputs, outputs=outputs)
model.compile(
    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
)

# The displayed loss will be much higher than before
# due to the regularization component.
model.fit(x_train, y_train, batch_size=64, epochs=1)    
```



```python
class MetricLoggingLayer(layers.Layer):
    def call(self, inputs):
        # The `aggregation` argument defines
        # how to aggregate the per-batch values
        # over each epoch:
        # in this case we simply average them.
        self.add_metric(
            keras.backend.std(inputs), name="std_of_activation", aggregation="mean"
        )
        return inputs  # Pass-through layer.
#std: 标准差

inputs = keras.Input(shape=(784,), name="digits")
x = layers.Dense(64, activation="relu", name="dense_1")(inputs)

# Insert std logging as a layer.
x = MetricLoggingLayer()(x)

x = layers.Dense(64, activation="relu", name="dense_2")(x)
outputs = layers.Dense(10, name="predictions")(x)

model = keras.Model(inputs=inputs, outputs=outputs)
model.compile(
    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
)
model.fit(x_train, y_train, batch_size=64, epochs=1)
```

#### Keras 顺序模型







创建顺序模型: 

```python
model = keras.Sequential(
    [
        layers.Dense(2, activation="relu"),
        layers.Dense(3, activation="relu"),
        layers.Dense(4),
    ]
)
```

或者使用add()创建模型

```python
model = keras.Sequential()
model.add(layers.Dense(2, activation="relu"))
model.add(layers.Dense(3, activation="relu"))
model.add(layers.Dense(4))
```

使用pop进行删除

```python
model.pop()
print(len(model.layers))  # 2
```

顺序构造函数可以命名, 用来注释TensorBoard会很方便

```python
model = keras.Sequential(name="my_sequential")
model.add(layers.Dense(2, activation="relu", name="layer1"))
model.add(layers.Dense(3, activation="relu", name="layer2"))
model.add(layers.Dense(4, name="layer3"))
```

预先指定输入形状

```python
layer = layers.Dense(3)
layer.weights  # Empty
```

```python
# Call layer on a test input
x = tf.ones((1, 4))
y = layer(x)
layer.weights  
# Now it has weights, of shape (4, 3) and (3,)
```

```python
model = keras.Sequential(
    [
        layers.Dense(2, activation="relu"),
        layers.Dense(3, activation="relu"),
        layers.Dense(4),
    ]
)  # No weights at this stage!

# At this point, you can't do this:
# model.weights

# You also can't do this:
# model.summary()

# Call the model on a test input
x = tf.ones((1, 4))
y = model(x)
print("Number of weights after calling the model:", len(model.weights))  # 6
```

![img](C:\Users\75451\Desktop\learning\2020_autumn\大数据与机器智能\pic\FqWkXBbOseaWE39njM9vClFseJik)

可以显示到目前为止模型的摘要 包括当前的输出形状

```python
model = keras.Sequential()
model.add(keras.Input(shape=(4,)))
model.add(layers.Dense(2, activation="relu"))

model.summary()


```

```python
model = keras.Sequential()
model.add(layers.Dense(2, activation="relu", input_shape=(4,)))

model.summary()
```



```python
model = keras.Sequential()
model.add(keras.Input(shape=(250, 250, 3)))  # 250x250 RGB images
model.add(layers.Conv2D(32, 5, strides=2, activation="relu"))
model.add(layers.Conv2D(32, 3, activation="relu"))
model.add(layers.MaxPooling2D(3))

# Can you guess what the current output shape is at this point? Probably not.
# Let's just print it:
model.summary()

# The answer was: (40, 40, 32), so we can keep downsampling...

model.add(layers.Conv2D(32, 3, activation="relu"))
model.add(layers.Conv2D(32, 3, activation="relu"))
model.add(layers.MaxPooling2D(3))
model.add(layers.Conv2D(32, 3, activation="relu"))
model.add(layers.Conv2D(32, 3, activation="relu"))
model.add(layers.MaxPooling2D(2))

# And now?
model.summary()

# Now that we have 4x4 feature maps, time to apply global max pooling.
model.add(layers.GlobalMaxPooling2D())

# Finally, we add a classification layer.
model.add(layers.Dense(10))
```

顺序模型功能提取: 

每一层都有一个输入输出属性, 快速创建一个模型, 提取序列模型中中间层的输出. 

##### 基于顺序模型的迁移学习



