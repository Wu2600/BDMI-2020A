# 课程总结

## **循环网络Recurrent Neutral Network**

**1. 循环网络基本结构**

前状态会反映到下一个时间步

权重共享Weight Share

计算方程：隐藏层为前一时刻隐藏层状态与输入的综合，激活函数后作为隐藏层输出，经过全连接层，之后softmax转化为向量，与标签对比

长短时记忆网络LSTM

门控循环单元网络GRU

问题：收敛慢，梯度消失或梯度爆炸

**2. 双向RNN**：语音识别，手写识别

**3.长短时记忆网络LSTM**

​		记忆单元、遗忘门、输入门、输出门

**4. GRU**

​		合并输入门、遗忘门为更新门，重置门

序列对序列模型：机器翻译->编码器、解码器结构

序列输出预测：注意力机制，利用前一段时间的信息，而不是前一个时间——Transformer模型，去掉RNN序列结构，优点是计算快，完全基于attention和全连接

**TensorFlow Keras RNN**

​		TensorFlow2 - Keras API

​		tf.keras.layers.RNN

​		make_csv_dataset