### Tensor的运算

```python
tf.add()
tf.multiply()
tf.matmul()
#张量运算符: 
tf.reduce_max()
tf.argmax(c)
tf.nn.softmax(c)

```

### Tensor形状

### Tensor索引

索引从0开始, 冒号用于切片, 索引为左闭右开;

负索引表示按照倒序编制索引

#### 单轴切片

#### 多轴切片

#### Tensor的reshape

错误的Tensor reshape使用: 

#### Tensor广播

为了适应大张量, 会对小张量进行拓展

#### 字符串张量

#### 稀疏张量

```python
#稀疏张量
sparse_tensor = tf.sparse.SparseTensor(indices = [[0,0],[1,2]],
                                      values = [1,2],
                                      dense_shape= [3,4])
print(sparse_tensor)
#SparseTensor(indices=tf.Tensor(
#[[0 0]
# [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))
print(tf.sparse.to_dense(sparse_tensor))
#tf.Tensor(
#[[1 0 0 0]
# [0 0 2 0]
# [0 0 0 0]], shape=(3, 4), dtype=int32)
```

### TensorFlow 变量

### TensorFlow自动微分

```python
# 自动微分(tf.GradientTape API)
x = tf.ones((2,2))
with tf.GradientTape() as t:
    t.watch(x)
    y = tf.reduce_sum(x)
    z = tf.multiply(y,y)
#dz_dx = t.gradient(z,x)
#print(dz_dx)
#for i in [0,1]:
#    for j in [0,1]:
#        assert dz_dx[i][j].numpy() == 8
dz_dy = t.gradient(z,y)
print(dz_dy)
assert dz_dy.numpy() == 8
# 自动微分(tf.GradientTape API)
x = tf.ones((2,2))
with tf.GradientTape() as t:
    t.watch(x)
    y = tf.reduce_sum(x)
    z = tf.multiply(y,y)
#dz_dx = t.gradient(z,x)
#print(dz_dx)
#for i in [0,1]:
#    for j in [0,1]:
#        assert dz_dx[i][j].numpy() == 8
dz_dy = t.gradient(z,y)
print(dz_dy)
assert dz_dy.numpy() == 8
tf.Tensor(8.0, shape=(), dtype=float32)
# 持续的梯度带
x = tf.constant(3.0)#为什么需要是float
with tf.GradientTape(persistent = True) as t:
    t.watch(x)
    y = x*x
    z = y*y
dz_dx = t.gradient(z,x)
dz_dy = t.gradient(z,y)
print(dz_dx)
print(dz_dy)
del t
tf.Tensor(108.0, shape=(), dtype=float32)
tf.Tensor(18.0, shape=(), dtype=float32)
1
def f(x):
    output = 1.0
    output = tf.multiply(x,x) + tf.exp(x)
    return output
x = tf.convert_to_tensor(1.0)
with tf.GradientTape(persistent = True) as t:
    t.watch(x)
    y  = f(x)
dy_dx = t.gradient(y,x)
print(dy_dx)
tf.Tensor(4.7182817, shape=(), dtype=float32)
d2y_dx2
#计算高阶微分
x = tf.Variable(1.0)
with tf.GradientTape() as t:
    with tf.GradientTape() as t2:
        y = x*x*x
    dy_dx = t2.gradient(y,x)
d2y_dx2 = t.gradient(dy_dx,x)
print(d2y_dx2)
tf.Tensor(6.0, shape=(), dtype=float32)
```

### TensorFlow 即刻执行

![img](C:/Users/75451/Desktop/learning/2020_autumn/大数据与机器智能/pic/FgZF_9uolBtfRzbh62xFouY0tXKI)

即刻求值与惰性求值

**=>即刻执行**

```python
x = [[2.]]
m = tf.matmul(x,x)
print("hello,{}".format(m))
#hello,[[4.]]
```

**即刻训练**

```python
#即刻训练
w = tf.Variable([[1.0]])
with tf.GradientTape() as tape:
    loss = w*w
grad = tape.gradient(loss,w)
print(grad)
tf.Tensor([[2.]], shape=(1, 1), dtype=float32)
```



## 第10节课

### TensorFlow计算图(Graph)

Graph是包含一系列TensorFlow操作的数据结构

图的优势: 灵活性强, 执行效率高(底层实际上是C), 容易优化(十分形象, 可以看出每个tensor是如何流动的)

#### 图的建立

tf.function可以递归地跟踪他调用的任何python函数

#### 图流控制

```python
def my_function(x):
  if tf.reduce_sum(x) <= 1:
    return x * x
  else:
    return x-1

a_function = tf.function(my_function)

print("First branch, with graph:", a_function(tf.constant(1.0)).numpy())
print("Second branch, with graph:", a_function(tf.constant([5.0, 5.0])).numpy())
--------------
#First branch, with graph: 1.0
#Second branch, with graph: [4. 4.]
```

#### 利用tensorboard显示图

### 模块 层 模型

**模块** : 对张量进行计算的函数

变量在训练过程中被更新

TensorFlow中定义模型和层: 

层: 可重用的带参数的结构

它们都是tf.Module的派生类

#### 模块

继承自(tf.Module)

实际上函数的功能只和call相关

\__call__函数:返回值 ax + b ; 

5*5+5 = 30

#### 查看可训练 不可训练变量

#### 从模块到层

层的特征需要定义 (in_feature, out_feature)

super() :  超类(tf.Module)

w b 是我们需要训练的量

w是3*1的正态分布矩阵

b为一个out_feature维的向量

```python
class Dense(tf.Module):
    def __init__(self, in_features = 3, out_features = 1, name=None):
        super().__init__(name=name)
        self.w = tf.Variable([[-3.14, -2.31, 2.16]], name='w')#w是一个3*1的张量
    def __call__(self, x):
        y = tf.matmul(self.w, x)
        return tf.nn.sigmoid(y)
```



#### 层组合得到模型

先经过第一层, 再经过第二层即可得到

#### 查看模型内的子模块与变量



```python
class Dense(tf.Module):
    def __init__(self, in_features, out_features, name=None):
        super().__init__(name=name)
        self.w = tf.Variable(tf.random.normal([in_features, out_features]), name='w')
        self.b = tf.Variable(tf.zeros([out_features]), name='b')
    def __call__(self, x):
        y = tf.matmul(x, self.w)+ self.b
        return tf.nn.sigmoid(y)
# 模型
class MyModule(tf.Module):
    def __init__(self, name=None):
        super().__init__(name = name)
        
        self.dense_1 = Dense(in_features=3, out_features=2)
        self.dense_2 = Dense(in_features=2, out_features=1)
        
    def __call__(self, x):
        x = self.dense_1(x)
        return self.dense_2(x)
myM = MyModule(name = 'mym')
print("result",myM(tf.constant([[0.0288, -0.3256, 0.5925]])))
```



#### 层的动态构建

#### 模型的恢复与储存



```python
chkp_path = 'my_checkpint'
checkpoint = tf.train.Checkpoint(model=myM)
checkpoint.write(chkp_path)
#'my_checkpint'
tf.train.list_variables(chkp_path)
# [('_CHECKPOINTABLE_OBJECT_GRAPH', []),
# ('model/dense_1/b/.ATTRIBUTES/VARIABLE_VALUE', [2]),
# ('model/dense_1/w/.ATTRIBUTES/VARIABLE_VALUE', [3, 2]),
# ('model/dense_2/b/.ATTRIBUTES/VARIABLE_VALUE', [1]),
# ('model/dense_2/w/.ATTRIBUTES/VARIABLE_VALUE', [2, 1])]

```



#### 可视化



```python
#set up logging 
stamp = datetime.now().strftime("%Y%m%d-%H%M%S")
logdir = "logs/func/%s" % stamp
writer = tf.summary.create_file_writer(logdir)

new_model = MyModule()

tf.summary.trace_on(graph=True, profiler=True)

z = print(new_model(tf.constant([[2.0,2.0,2.0]])))
with writer.as_default():
    tf.summary.trace_export(
    name = 'my_func_trace',
    step=0,
    profiler_outdir=logdir)
    
---
#然后输入
%load_ext tensorboard
%tensorboard --logdir logs/func
```

#### 存储



```python
tf.saved_model.save(myM, 'the_saved_model')
#INFO:tensorflow:Assets written to: #the_saved_model\assets

#ls -l the_saved_model
#ls -l the_saved_model/variables
new_model = tf.saved_model.load('the_saved_model')
isinstance(new_model, SimpleModule)
False
print(myM([[3.0,3.0,3.0]]))
tf.Tensor([[0.49567753]], shape=(1, 1), dtype=float32)
3
print(myM([[[3.0,3.0,3.0], [3.0,3.0,3.0]]]))
tf.Tensor(
[[[0.49567753]
  [0.49567753]]], shape=(1, 2, 1), dtype=float32)
```



### TensorFlow训练流程



#### 1 获取数据

f(x) = x*W + b

W为权重, b为偏置

#### 2 定义模型

#### 3 定义损失函数

#### 4 运行训练数据, 从目标值计算损失

#### 5 调整变量

#### 6 结果评估

