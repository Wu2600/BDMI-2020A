## 课程笔记

## 循环网络recurrent networks

循环网络——反馈网络，增加了一个时间维度

特点：在时间维度上每一个时间步处理时，采用相同的权重（权重共享）

### 基本循环网络 vanilla RNN

反馈连接：隐藏层的当前状态，会反馈到下一个时间步的状态

*隐藏层的神经元个数是循环网络的一个参数



###循环网络训练

###——通过时间反向传播BPTT

### 循环网络的变种

LSTM（长短时记忆网络）解决基本循环网络收敛慢的问题

GRU（门控循环单元网络）解决LSTM的计算成本高的问题

### 双向循环网络

一个时间上从序列七点开始移动的RNN和另一个时间上从序列末尾开始移动的RNN

### 长短时记忆网络

门结构：输入和控制是形状一致的张量控制经过sigmoid函数后编程一个范围在0-1的张量

LSTM增加了一个记忆单元C、遗忘门f、输入门i、输出门o

<img src="C:\Users\回忆再美好也只是曾经\AppData\Roaming\Typora\typora-user-images\image-20201208143443602.png" alt="image-20201208143443602" style="zoom:67%;" />

ft是对上一个时刻的状态进行部分的丢弃

<img src="https://qn-st0.yuketang.cn/Fp6z83Nql-gf9AbcYTDjHTF8K1yt" alt="img" style="zoom:67%;" />

<img src="https://qn-st0.yuketang.cn/Fvc6aR2QCyVQDQTVznpQoZM8b5Jb" alt="img" style="zoom:67%;" />

<img src="https://qn-st0.yuketang.cn/FgvRmNK3dzWECnU9XIUswbpfyuVE" alt="img" style="zoom:67%;" />

### GRU

只有两个门：

更新门：合并了LSTM的输入门和遗忘门

重置门：重置门关闭会忽略掉历史信息

GRU只有一个状态：合并了LSTM的记忆状态C和隐藏状态H为一个状态



###循环网络应用

语音识别

图片注解

单词嵌入向量

文本分类

文本生成

### 序列对序列模型

神经网络机器翻译NMT

### 序列输出预测

注意力机制：把中间状态记录下来

### 循环网络扩展

神经图灵机

### tf.keras.layers.rnn

rnn-cell抽象：当前的输出完全取决于state和当前的输入

Cell 参数：num_units神经元的个数

#### 

