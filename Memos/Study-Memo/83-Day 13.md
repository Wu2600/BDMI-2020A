# 83 Day 13
### 课程总结：
这节课主要介绍了循环网络（RNN）的基本概念与两个延伸优化网络LSTM与GRU以及在keras中的RNN实现，由此节课掌握了机器学习中另一个主要的网络模型，从而学会了如何使用神经网络进行图片（CNN）与文本/语音（RNN）的识别任务。RNN类似回归中的时间序列自回归
### 学习总结：

| 基本循环网络（Vanilla RNN） | RNN是基于时间反向传播算法（BPTT）的一种反馈网络，即在时间维度上，每个节点的输出能够作为另一个节点的输入（前向或后向），每个时间步在处理时会有相同的权重。Vanilla RNN指的是在传统的x(输入层)->h(隐藏层)->o（输出）->计算L(Loss) 一个三层全连接网络, 但是在h处引入了一个反馈W, 造成了时间上的反馈，从而在每个时间节点中，将输入的W反馈与输入序列映射到输出序列o，将o输入进L得到最终的输出y，将y与期望的输出序列进行Loss计算，从而基于BPTT在每一个时间都调整一次权重，直到训练结束。RNN的作用是在时间维度中给定个数据，网络预测接下来要输入的值是什么。例如预测hello中某个字母的下一个字母是什么，或是基于今天的天气预测明天的天气等。其主要用途包括单词嵌入向量（单词放在向量里），图片注解（一张图片上自动分类），文本分类（好评/差评？），文本生成（写诗）等 |
| :---------------: | :----------------------------------------------------------- |
| LSTM与GRU网络 | 长短时记忆网络LSTM与其简化的门控循环单元网络GRU是为了解决由于BPTT训练RNN时每个时间步只能一前一后地计算，内存代价是O（T），训练也很慢，会出现梯度消失和梯度爆炸的问题，虽然基本循环网络也能用梯度截断来解决初步和终步的时间梯度太大或太小的问题，但是直接应用这两类变种RNN的效果更有保障。LSTM具有记忆单元，输入门、遗忘门、输出门，能够解决基本循环网络收敛慢的问题。GRU合并了输入门和遗忘门，用于控制历史信息的影响；同时还有重置门，可以忽略历史信息。GRU相比LSTM更为简洁，需要的计算量更小。 |
| Keras中的RNN实现 | 基本RNN：tf.keras.layers.RNN<br />双向RNN：tf.keras.layers.Bidirectional<br />LSTM（实际应用最广）：tf.keras.layers.LSTM<br />GRU：tf.keras.layers.GRU<br />双向LSTM:tf.keras.layers.Bidirectional(tf.keras.layers.LSTM)<br />使用tf.keras.layers.RNN(tf.keras.layers.GRUCell)和直接使用tf.keras.layers.GRU的效果是相同的 |

### 训练代码：

