# 83 Day 12
### 课程总结：
这节课主要介绍了卷积网络（CNN）的结构与应用，keras模型的保存与加载，数据集的制作，以及keras中常用的模型网络与层，由这节课更能感受到keras的灵活性，并关于神经网络的搭建从数据集的准备到最终的训练与模型保存这个全套流程有了系统性的理解（如鲜花分类例子）
### 学习总结：

| 卷积网络（CNN） | 卷积网络属于前馈网络，并且卷积网络是局部连接, 不是全连接，其是使用特定大小的卷积核，在图片中按照设定的步长依次移动，每移动一次获奖卷积核覆盖范围内的矩阵进行点乘输出，它具有计算快，参数小，可以并行化的特点，并且既可以应用在2D的平面，也可以用在三维的图片（包含RGB颜色通道）中。卷积核的权重是不变的，而每个卷积核注重的特征是不同的，因此不能用单个卷积核来判断整张图片的各个位置的特征，因此需要用多个卷积核来进行卷积计算，从而捕捉到图片的尽量多的特征。模型参数查看可以用工具netron:https://github.com/lutzroeder/netron |
| :---------------: | :----------------------------------------------------------- |
| keras模型的保存与加载 | 使用model.save()与keras.models.load()可以进行模型的保存与加载，这种方法不仅保存了模型的权重，还将模型的架构与配置一起保存，若想分开保存，则可以使用get_config()/from_config()来保存架构，或是model.save_weights/model.load_weights来保存权重 |
| 数据流水线 | 若不是用高级的API接口，可以使用tf.data从不同的数据格式中读取数据（list，numpy，图片，文本，TFRecord数据，表格）、加载数据，打乱数据，数据预处理。其中tf.data.Dataset用于表示序列的元素，每个元素就是一个基本的训练样本，若使用from_tensor_slices，则必须先用张量表示图像和对应的标签。tf.data.Dataset.from_generator可以减少内存开销，直接由生成函数生成数据。更为高级的API接口可以从TensorFlow官网中或是课件中查询。 |
| 模型网络与层 | 下采样层(池化)，作用: 缩小输出张量大小1 最大池化（Maxpooling） 2 平均值化（Averagepooling）<br />归一化层(正则化层)LCN层：LCN的目标是减去平均值, 除以标准差特点是亮度不变, 对图像识别用处很大，用于减小过拟合，但是也可以用dropour或者LRN层，因为keras中貌似没有LCN的layer<br />激活函数层：如softmax，sigmoid，relu等，主要是用来增加模型的非线性表达能力，或是使函数以目标形式（概率/分类等）输出在目的区间范围内但是在其它层中也能应用激活函数<br />密集层（Dense）：密集层也是用来增加模型的非线性表达能力，它是将前面提取的特征，在dense经过非线性变化，提取这些特征之间的关联，最后映射到输出空间。密集层能够增加模型的复杂度，从而增加模型的学习能力，但是太多也容易造成过拟合和效率变低的问题，所以要适当选择才能让模型训练中收敛的最快<br />随机丢弃层（dropuout）：设置丢弃率，或者保留率（keep_prob）来使层的输出权重随机置为0，从而改善由于数据量太小，模型识别到不必要的噪音而造成的过拟合现象。 |

### 训练代码：

[Day 12]: https://github.com/Transparent-Boy/Practice-code-along-with-the-class/tree/main/Day%2012

