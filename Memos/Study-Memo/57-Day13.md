### 第十三次课程小结

#### 循环网络(Recurrent Networks)

##### RNN

- 时间维度上的权重共享

##### vanilla RNN

- 梯度截断

##### LSTM，长短时记忆网络

解决vanilla RNN收敛慢的问题

##### GRU，门控循环单元网络

解决LSTM计算成本高的问题

##### 门结构(Gate)

- 输入（Input）和控制（Control）是形状一致的张量（Tensor）
- 控制经过Sigmoid函数后，变成一个范围在0~1之间的一个同形状的张量
- 输入和σ（Control）元素相乘，得到一个同形的输出（Output）

##### LSTM（Long-short term memory）

增加了一个主输入单元和其他三个辅助的门限输入单元

- 记忆单元（memory cell）
- 输入门（input gate）、遗忘门（forget gate）、输出门（output gate）

解决RNN收敛慢的问题，提升了RNN处理远距离依赖问题的能力

##### GRU(Gated Recurrent Unit)

GRU单元只有两个门

- 更新门（update gate），合并了LSTM的输入门和遗忘门，用于控制历史信息对当前时刻隐藏层输出的影响。如果更新门接近1，会把历史信息传递下去。
- 重置门（reset gate），如果重置门关闭，会忽略历史信息，及历史不相干的信息不会影响未来的输出。

GRU只有一个状态

- 合并LSTM的记忆状态C（cell state）和隐藏状态H（hidden state）为一个状态（基于经验的观察）

#### 循环网络的Keras表达

RNN训练有以下问题：

- 梯度爆炸
- 梯度消失
- 收敛慢

LSTM解决以上问题，在实际中应用最广。 

