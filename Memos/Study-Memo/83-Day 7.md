# 83 Day 7
### 课程总结：
这节课开始进入到机器学习篇章，首先是作为Python数学计算应用最为广泛的包Numpy的讲解，不仅接触了一些基础的numpy计算语言（元素计算），还学到了类似dot，matmul等矩阵的线性代数计算函数，还有其他类似transpose，ravel，h/vstack等关于数组形状的改变方式，在讲解完Numpy，就进入到机器学习理论层面的介绍，诸如激活函数（逻辑斯提回归单元），神经元模型，以及将其应用于布尔计算的模型（与/非/或门等）等，最终还引入了张量神经网络的概念
### 学习总结：

| Numpy | 接触了一些基础的numpy计算语言（元素计算），还学到了类似dot，matmul等矩阵的线性代数计算函数，还有其他类似transpose，ravel，h/vstack等关于数组形状的改变方式 | 全面了解了Numpy在神经网络中的主要应用及其数学计算函数 |
| ----------------- | ------------------------------------------------------------ | ------------------------------------ |
| 激活函数（逻辑斯提回归单元） | 学习了各种激活函数（逻辑斯提回归单元），主要有sigmoid、tanh、ReLU，它们都是在（0，1）区间内建立条极值在（0，1）区间的方程，输入的变量经权重的改变再进入这个激活函数即能得出最终的神经元输出结果 | 了解了激活函数的作用与机制 |
| 电路式的神经网络（每个单元只有0、1两个输出结果，主要应用有布尔计算） | 学习了AND,OR,NOT,NAND,XOR几个布尔计算如何用人工神经元建立相应的模型 | 了解了人工神经元的实际输出形式 |
| 神经网络的其他组成 | 学习了如softmax等归一化的函数来将输出结果或数据转化为（0，1）区间的概率，从而避免输出结果在最终的判断依据区间之外，以及logit这个函数，其是sigmoid的反函数，其目的是为了把由这类函数所得出的概率反推出映射到（∞，∞），还有类似均方误差与交叉熵等用于机器学习阶段的损失函数的模型 | 关于神经网络的整体思路与机制有了深入的了解 |

### 训练代码：

[Day 7]: https://github.com/Transparent-Boy/Practice-code-along-with-the-class/tree/main/Day%207

