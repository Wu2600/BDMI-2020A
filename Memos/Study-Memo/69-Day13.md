# 第十三周



# 循环网络

循环网络基本结构：

 时间维度上，每个时间步处理时，采用相同的权重

## vanilla RNN

训练目标：y

损失函数：L

输出层：o

隐藏层：h

输入层

Grandient Clipping

梯度过大时，将梯度固定

LSTM

解决收敛慢的问题；最常用

GRU

解决LSTM计算成本高的问题

### 双向循环网络

## LSTM

### Gate

${\rm output} = {\rm input}*\sigma({\rm control})$ 可反向传播训练

### LSTM结构

记忆单元；输入门，遗忘门，输出门

## GRU

LSTM的简化 合并输入门和遗忘门，只有更新门和重置门 合并记忆状态和隐藏状态

## RNN应用

手写识别；机器翻译

### 注意力机制

 将之前的状态再加一层全连接，输出作为最终输出的部分计算参数

transformer模型

