# 第十三周小结

## 循环网络

LSTM（长短时记忆网络）：相比较RNN，增加了输入门、遗忘门、输出门

GRU（门控循环单元网络）

### 双向循环网络

正向 & 反向同时传播



更详细的课外学习：

simple RNN只加入记忆单元，每一次的输入由记忆单元和当前输入共同决定，RNN一般不用ReLU

RNN的训练中梯度很诡异，需要进行clipping，限制梯度最大值，避免nan

(例如一个循环1000次的网络，假设输出为$w^{1000}$，当$w=1.01$时$y\approx 20000$，梯度很大，而w取0.01或0.99时y区别不大，梯度很小）

LSTM使用门结构，由sigmoid和点乘组合而成，输入通过sigmoid函数后转换为0到1的数值，表示门开启的程度

有三种门：输入(i)、输出(o)、遗忘门(f)，$x_t$和$y_{t-1}$通过线性变换得到$x_i,x_o,x_f,x$ ,向量维数为神经元的个数（每一维进入一个神经元）

$c_t = i(x_i) * x + f(x_t) * c_{t-1}$ 

$y_t = o(x_o) * c_t$

常用的变式在LSTM的结构中加入了“peephole connections”结构，输入除了$x_t$和$y_{t-1}$还加入了$c_{t-1}$

LSTM可以解决梯度消失（除非遗忘门被关闭），而不能解决梯度爆炸，所以learning rate要设得很小

如果LSTM的overfitting很严重，可以尝试更换成GRU，GRU参数量更少（只有两个门，相当于把输入门和遗忘门联动），且效果近似。

