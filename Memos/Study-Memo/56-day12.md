# 第12节课

## Keras学习

保存和加载模型

模型可以在训练期间和训练完成后进行保存。这意味着模型可以从任意中断中恢复，并避免耗费比较长的时间在训练上。保存也意味着您可以共享您的模型，而其他人可以通过您的模型来重新创建工作。在发布研究模型和技术时，大多数机器学习从业者分享：

- 用于创建模型的代码
- 模型训练的权重 (weight) 和参数 (parameters) 。

共享数据有助于其他人了解模型的工作原理，并使用新数据自行尝试。

注意：小心不受信任的代码——Tensorflow 模型是代码。有关详细信息，请参阅 [安全使用Tensorflow](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md)。

### 选项

保存 Tensorflow 的模型有许多方法——具体取决于您使用的 API。本指南使用 [tf.keras](https://tensorflow.google.cn/guide/keras?hl=zh_cn)， 一个高级 API 用于在 Tensorflow 中构建和训练模型。有关其他方法的实现，请参阅 TensorFlow [保存和恢复](https://tensorflow.google.cn/guide/saved_model?hl=zh_cn)指南或[保存到 eager](https://tensorflow.google.cn/guide/eager?hl=zh_cn#object-based_saving)。

### 安装并导入

安装并导入Tensorflow和依赖项：

```bsh
pip install -q pyyaml h5py  # 以 HDF5 格式保存模型所必须
WARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.
You should consider upgrading via the '/tmpfs/src/tf_docs_env/bin/python -m pip install --upgrade pip' command.
import os

import tensorflow as tf
from tensorflow import keras

print(tf.version.VERSION)
2.3.0
```

### 获取示例数据集

要演示如何保存和加载权重，您将使用 [MNIST 数据集](http://yann.lecun.com/exdb/mnist/). 要加快运行速度，请使用前1000个示例：

```python
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

train_labels = train_labels[:1000]
test_labels = test_labels[:1000]

train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0
test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0
```

### 定义模型

首先构建一个简单的序列（sequential）模型：

```python
# 定义一个简单的序列模型
def create_model():
  model = tf.keras.models.Sequential([
    keras.layers.Dense(512, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10)
  ])

  model.compile(optimizer='adam',
                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

  return model

# 创建一个基本的模型实例
model = create_model()

# 显示模型的结构
model.summary()
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 512)               401920    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                5130      
=================================================================
Total params: 407,050
Trainable params: 407,050
Non-trainable params: 0
_________________________________________________________________
```

### 在训练期间保存模型（以 checkpoints 形式保存）

您可以使用训练好的模型而无需从头开始重新训练，或在您打断的地方开始训练，以防止训练过程没有保存。 [`tf.keras.callbacks.ModelCheckpoint`](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks/ModelCheckpoint?hl=zh_cn) 允许在训练的*过程中*和*结束时*回调保存的模型。

### Checkpoint 回调用法

创建一个只在训练期间保存权重的 [`tf.keras.callbacks.ModelCheckpoint`](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks/ModelCheckpoint?hl=zh_cn) 回调：

```python
checkpoint_path = "training_1/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# 创建一个保存模型权重的回调
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

# 使用新的回调训练模型
model.fit(train_images, 
          train_labels,  
          epochs=10,
          validation_data=(test_images,test_labels),
          callbacks=[cp_callback])  # 通过回调训练

# 这可能会生成与保存优化程序状态相关的警告。
# 这些警告（以及整个笔记本中的类似警告）
# 是防止过时使用，可以忽略。
```

保存和加载权重值

## 多层神经网络

### BP算法

前向传播是指从输入逐层计算，得到输出。反向传播用来对参数进行梯度计算和更新。

为了简化，将这些层全部考虑为全连接层，这样每层都有w和b参数矩阵。过段时间再重新推导一遍。在推导过程中，首先要明确，z=w*x+b, a=g(z)。其中，x，a分别指改层的输入和输出，g(z)是指激活函数。这样在推导的过程中就会非常明确，思路清晰。

![img](https://img-blog.csdnimg.cn/20190312113148461.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0lCZWxpZXZlMjAxNg==,size_16,color_FFFFFF,t_70)

### 卷积网络的基本结构

**卷积层（convolutional layer）**

*1.* *[卷积核](https://baike.baidu.com/item/卷积核/3377590)（convolutional kernel）*

卷积层的功能是对输入数据进行特征提取，其内部包含多个卷积核，组成卷积核的每个元素都对应一个权重系数和一个偏差量（bias vector），类似于一个前馈神经网络的神经元（neuron）。卷积层内每个神经元都与前一层中位置接近的区域的多个神经元相连，区域的大小取决于卷积核的大小，在文献中被称为“[感受野](https://baike.baidu.com/item/感受野/8989338)（receptive field）”，其含义可类比[视觉皮层](https://baike.baidu.com/item/视觉皮层/10986729)细胞的感受野 [2] 。卷积核在工作时，会有规律地扫过输入特征，在感受野内对输入特征做矩阵元素乘法求和并叠加偏差量 [1] ：

![img](https://bkimg.cdn.bcebos.com/formula/eb0088f908d955e0e452d3b05ae309b4.svg)

![img](https://bkimg.cdn.bcebos.com/formula/1375ccf3da0c734269f31499744ad0e9.svg)

式中的求和部分等价于求解一次[交叉相关](https://baike.baidu.com/item/交叉相关/5024000)（cross-correlation）。

 是卷积层参数，对应卷积核大小、卷积步长（stride）和填充（padding）层数 。

上式以二维卷积核作为例子，一维或三维卷积核的工作方式与之类似。理论上卷积核也可以先翻转180度，再求解交叉相关，其结果等价于满足交换律的[线性卷积](https://baike.baidu.com/item/线性卷积/5908978)（linear convolution），但这样做在增加求解步骤的同时并不能为求解参数取得便利，因此线性卷积核使用交叉相关代替了卷积 [4] [16] 。

 且不包含填充的单位卷积核时，卷积层内的交叉相关计算等价于矩阵乘法，并由此在卷积层间构建了全连接网络 [2] ：

![img](https://bkimg.cdn.bcebos.com/formula/22e4de52c5bc38148a6011b703f36f02.svg)

由单位卷积核组成的卷积层也被称为网中网（Network-In-Network, NIN）或多层感知器卷积层（multilayer perceptron convolution layer, mlpconv） [27] 。单位卷积核可以在保持特征图尺寸的同时减少图的通道数从而降低卷积层的计算量。完全由单位卷积核构建的卷积神经网络是一个包含参数共享的[多层感知器](https://baike.baidu.com/item/多层感知器/10885549)（Muti-Layer Perceptron, MLP） [27] 。

在线性卷积的基础上，一些卷积神经网络使用了更为复杂的卷积，包括平铺卷积（tiled convolution）、[反卷积](https://baike.baidu.com/item/反卷积/2281313)（deconvolution）和扩张卷积（dilated convolution） [2] 。平铺卷积的卷积核只扫过特征图的一部份，剩余部分由同层的其它卷积核处理，因此卷积层间的参数仅被部分共享，有利于神经网络捕捉输入图像的旋转不变（shift-invariant）特征 [28] 。[反卷积](https://baike.baidu.com/item/反卷积/2281313)或转置卷积（transposed convolution）将单个的输入激励与多个输出激励相连接，对输入图像进行放大。由反卷积和向上池化层（up-pooling layer）构成的卷积神经网络在图像语义分割（semantic segmentation）领域有应用 [29] ，也被用于构建卷积自编码器（Convolutional AutoEncoder, CAE） [30] 。扩张卷积在线性卷积的基础上引入扩张率以提高卷积核的感受野，从而获得特征图的更多信息 [31] ，在面向序列数据使用时有利于捕捉学习目标的长距离依赖（long-range dependency）。使用扩张卷积的卷积神经网络主要被用于[自然语言处理](https://baike.baidu.com/item/自然语言处理/365730)（Natrual Language Processing, NLP）领域，例如[机器翻译](https://baike.baidu.com/item/机器翻译/411793) [31] 、[语音识别](https://baike.baidu.com/item/语音识别/10927133) [32] 等。

*2. 卷积层参数*

[![卷积核中RGB图像的按0填充](https://bkimg.cdn.bcebos.com/pic/c2fdfc039245d688e3b60de4a9c27d1ed21b243d?x-bce-process=image/resize,m_lfit,w_220,limit_1)](https://baike.baidu.com/pic/卷积神经网络/17541100/0/c2fdfc039245d688e3b60de4a9c27d1ed21b243d?fr=lemma&ct=single)卷积核中RGB图像的按0填充 

卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数 [1] 。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂 [1] 。

卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素 [33] 。

由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充（replication padding）。填充依据其层数和目的可分为四类 [33] ：

- 有效填充（valid padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积（narrow convolution）”，窄卷积输出的特征图尺寸为(L-f)/s+1。
- 相同填充/半填充（same/half padding）：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积（equal-width convolution）”。
- 全填充（full padding）：进行足够多的填充使得每个像素在每个方向上被访问的次数相同。步长为1时，全填充输出的特征图尺寸为L+f-1，大于输入值。使用全填充的卷积被称为“宽卷积（wide convolution）”
- 任意填充（arbitrary padding）：介于有效填充和全填充之间，人为设定的填充，较少使用。

带入先前的例子，若16×16的输入图像在经过单位步长的5×5的卷积核之前先进行相同填充，则会在水平和垂直方向填充两层，即两侧各增加2个像素（

![img](https://bkimg.cdn.bcebos.com/formula/4952814c8e09026647103f75e0690709.svg)

 ）变为20×20大小的图像，通过卷积核后，输出的特征图尺寸为16×16，保持了原本的尺寸。

*3.* *[激励函数](https://baike.baidu.com/item/激励函数/6477658)**（activation function）*

卷积层中包含激励函数以协助表达复杂特征，其表示形式如下 [1] ：

![img](https://bkimg.cdn.bcebos.com/formula/61db66544976809c7cf7f30e21f3843b.svg)

类似于其它深度学习算法，卷积神经网络通常使用[线性整流函数](https://baike.baidu.com/item/线性整流函数/20263760)（Rectified Linear Unit, ReLU），其它类似ReLU的变体包括有斜率的ReLU（Leaky ReLU, LReLU）、参数化的ReLU（Parametric ReLU, PReLU）、随机化的ReLU（Randomized ReLU, RReLU）、指数线性单元（Exponential Linear Unit, ELU）等 [2] 。在ReLU出现以前，[Sigmoid函数](https://baike.baidu.com/item/Sigmoid函数/7981407)和[双曲正切函数](https://baike.baidu.com/item/双曲正切函数/15469414)（hyperbolic tangent）也有被使用 [15] 。

激励函数操作通常在卷积核之后，一些使用预激活（preactivation）技术的算法将激励函数置于卷积核之前 [34] 。在一些早期的卷积神经网络研究，例如LeNet-5中，激励函数在池化层之后 [5] 。

**池化层（pooling layer）**

在卷积层进行特征提取后，输出的特征图会被传递至池化层进行特征选择和信息过滤。池化层包含预设定的池化函数，其功能是将特征图中单个点的结果替换为其相邻区域的特征图统计量。池化层选取池化区域与卷积核扫描特征图步骤相同，由池化大小、步长和填充控制 [1] 。

### 卷积网络的应用

Keras常用网络结构——卷积层、池化层、随机丢弃

图像识别案例

数据集制作方法

