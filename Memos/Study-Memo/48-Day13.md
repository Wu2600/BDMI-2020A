## 第十三次课

### 循环网络的结构

#### 基本概念

记忆网络

循环网络

基本循环网络

……



单词嵌入向量

文本生成

文本分类

序列对序列模型



### 人工神经网络

人工神经元

多层人工神经网络

卷积网络



### 循环网络

基本循环网络（vanilla RNN）

![img](https://qn-st0.yuketang.cn/Fi-RMxU-WmdnoJ7VV--V583OuZEy)

权重共享

循环网络的视角：隐藏层的神经元个数

循环网络的计算方程

a（t） = b+Wh(t-1)+Ux(t)

全连接网络加上时间维度就变成了基本循环网络

#### 梯度截断



#### 循环网络的变种

LSTM：解决vanilla mn收敛慢的问题

GRU解决LSTM计算成本高的问题



### 门结构

输入和控制是形状一致的张量

门结构是可以通过反向传播训练的



### 长短时记忆网络

记忆单元

遗忘门

输入门

输出门



### GRU

更新门：合并了输入门和遗忘门

合并记忆状态和隐藏状态



### 循环网络应用

手写识别，诗歌填词，对联撰写，语音识别……

图片注解

单词嵌入向量

文本生成

网络机器翻译NMT：2个多层的LSTM

序列输出预测：greedy search-》注意力机制改进

transformer模型



### 循环网络的扩展

神经图灵机

可微神经计算机DNC



### keras RNN-1

所有RNN都进行了API化

RNN

#### rnn-cell抽象

hidden-units模型的容量大小

![img](https://qn-st0.yuketang.cn/FjvbnajALXb6s4Y88Cu0-LxKPeO4)

LSTM

tf.keras.layers.LSTM



### keras RNN-2

易用性：内置RNN

易于定制

cell级API

跨批次状态



RNN状态重用



### 大作业

时频谱图：24句，对其进行分类

搭网络进行训练

dense 24 进行分类得到结果

