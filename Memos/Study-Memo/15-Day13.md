# [练习链接](https://github.com/wsp1911/BDMI_class_practice/tree/master/13)
# 循环网络
时间维度上，每个时间点权重相同，称为权重共享
## vanilla RNN
* 训练目标：y
* 损失函数：L
* 输出层：o
* 隐藏层：h
* 输入层：x
$O^{(t)}=c+f(b+V(W h^{(t-1)}+U x^{(t)}))$
不同时间步上，采用相同的U,W,V矩阵
### BPTT
时间代价和空间代价都是O(T)
### 梯度截断（Grandient Clipping）
梯度过大时，将梯度固定
### LSTM
解决收敛慢的问题；最常用
### GRU
解决LSTM计算成本高的问题
### 双向循环网络
## LSTM
### Gate
${\rm output} = {\rm input}*\sigma({\rm control})$
可反向传播训练
### LSTM结构
记忆单元；输入门，遗忘门，输出门
## GRU
LSTM的简化
合并输入门和遗忘门，只有更新门和重置门
合并记忆状态和隐藏状态
## RNN应用
手写识别；诗歌填词，代码生成，对联撰写；股价预测，天气预测；语音识别，图片注释，机器翻译
### word embedding
one-hot
相似的词有相近的编码
## Seq2Seq
机器翻译：编码器和解码器
两个多层LSTM，一个作为编码器，一个作为解码器
### 注意力机制
greedy search效果不好
把以前的状态全连接生成注意力向量，作为输出层输入
### Transformer
完全基于attention和全连接
位置编码标记单词的位置
## 神经图灵机
* 控制器
* 外部记忆
* 读写操作
* 输入输出
# Tensorflow-RNN
* tf.keras.layers.SimpleRNNCell
* tf.keras.layers.SimpleRNN
* tf.keras.layers.RNN
* tf.keras.layers.Bidirectional
* tf.keras.layers.LSTMCell
* tf.keras.layers.LSTM
* tf.keras.layers.GRUCell
* tf.keras.layers.GRU
# tf.data
```python
tf.data.experimental.sample_from_datasets
```