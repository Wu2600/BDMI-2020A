### 第13周学习总结

#### 循环网络(RNN)

- 注意循环网络(RNN)展开之后会是什么样子，得到展开之后的计算图，并使用 Tensorflow 进行计算
- 核心的反向传播计算还是在矩阵上(但是不同时刻的UVW矩真均为同一个值)
- 全连接网络右旋90°之后就可能是循环网络
- 训练损失：将输入序列(x)映射到对应的输出序列(o)，损失L:衡量每个输出o与响应的训练目标y的距离
- 训练会慢一些(不像卷积核可以并行)  梯度爆炸与梯度消失:梯度趋于0或者十分大，固定设置梯度为1(梯度截断)
- 循环网络变种:长短时记忆网络(LSTM，解决基本循环网络收敛慢的问题)，门控循环单元网络(GRU，解决LST,计算成本高的问题)，双向循环网络

##### 长短时记忆网络

- 基础:门结构，本质上是循环网络的改进 增加了一个主输入单元 记忆单元
- 输入门、遗忘门、输出门
- 更加简化: GRU 只有 更新门 和 重置
- 手写识别、诗歌填词、股价预测、语音识别、图片注释、机器翻译

##### 序列对序列模型

编解码模型: 2个多层的 LSTM 网络 ； 1个Deep LSTM 将输入序列映射为一个固定维度的向量，另一个 Deep LSTM 将该向量解码为目标的序列

#### Keras的API

### tf.data  上次课程还没讲的东西

- 加载数据和标签(图片、列表等)，数据切片；(读取和切片)；
- 类别不平衡数据处理，读取图片和确认标签（注意API的使用）

- [代码](https://github.com/lixinyu0321/BDMI_mycode/tree/master/day13)

